{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Run SuperPrompt-v1 AI Model Gradio UI\n",
        "\n",
        "Make your prompts better for AI Art or in general!\n",
        "\n",
        "[Used Model](https://huggingface.co/roborovski/superprompt-v1)\n",
        "\n",
        "[Model Blog](https://brianfitzgerald.xyz/prompt-augmentation/)\n",
        "\n",
        "Task Prefix: \"Expand the following prompt to add more detail:\" is already setted!\n",
        "\n",
        "Google Colab Notebook Made by [Nick088](https://linktr.ee/Nick088) using Gradio UI (COULD RISK GOOGLE FREE TIER ACC)"
      ],
      "metadata": {
        "id": "rhAucpX5Li7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install & Load Dependencies, Model\n",
        "\n",
        "#@markdown If you wanna use CPU (slower, max 12 free hours daily limit): Set the CPU from Edit -> Notebook Settings -> CPU\n",
        "\n",
        "#@markdown If you wanna use GPU (faster, max 12 free hours daily limit): Set the Video Card from Edit -> Notebook Settings -> T4 GPU OR Any other GPUs based on your Google Colab Subscription\n",
        "\n",
        "#@markdown Anyways its a very small model, it doesn't matter much if you use cpu or gpu.\n",
        "\n",
        "import torch\n",
        "from IPython.display import clear_output\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    print(\"Using GPU\")\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "!git clone https://huggingface.co/spaces/Nick088/SuperPrompt-v1\n",
        "%cd SuperPrompt-v1\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "clear_output()\n",
        "print(f\"Downloaded & SuperPrompt-v1 on {'GPU' if device == 'cuda' else 'CPU'}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "CJKumDcSxPhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run UI\n",
        "%cd SuperPrompt-v1\n",
        "\n",
        "\n",
        "#@markdown The type of tunnel you wanna use for seeing the public link, so if the API of one of them is down, you can use the other one.\n",
        "Tunnel = \"LocalTunnel\" #@param [\"Gradio\", \"Ngrok\", \"Cloudfare\", \"LocalTunnel\"]\n",
        "\n",
        "#@markdown Also when using Ngrok, Cloudfare or LocalTunnel as the Tunnel, you need to wait for the Local URL to appear, and only after that click on the  Public URL above it.\n",
        "\n",
        "#@markdown Use the option under this only if you chose Ngrok as the Tunnel:\n",
        "\n",
        "#@markdown You can get the Ngrok Tunnel Authtoken here: https://dashboard.ngrok.com/tunnels/authtokens/new.\n",
        "\n",
        "ngrok_tunnel_authtoken = \"\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "file_path_ui = \"/content/SuperPrompt-v1/app.py\"\n",
        "\n",
        "\n",
        "\n",
        "if Tunnel == \"Gradio\":\n",
        "  !sed -i 's/share=False/share=True/g' $file_path_ui\n",
        "elif Tunnel == \"Ngrok\":\n",
        "  !sed -i 's/share=True/share=False/g' $file_path_ui\n",
        "  !pip install pyngrok\n",
        "  from pyngrok import ngrok\n",
        "  ngrok.set_auth_token(ngrok_tunnel_authtoken)\n",
        "  http_tunnel = ngrok.connect(7860, bind_tls=True)\n",
        "  clear_output()\n",
        "  print(\"Ngrok Tunnel Public URL:\", http_tunnel.public_url)\n",
        "elif Tunnel == \"Cloudfare\":\n",
        "  !sed -i 's/share=True/share=False/g' $file_path_ui\n",
        "  # download cloudfare\n",
        "  !curl -LO https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
        "  !dpkg -i cloudflared-linux-amd64.deb\n",
        "  !rm -rf nohup.out\n",
        "  import time\n",
        "  # Run cloudflare\n",
        "  !nohup cloudflared tunnel --url localhost:7860 &\n",
        "  clear_output()\n",
        "  time.sleep(5)\n",
        "  # Find and print the Cloudflare URL with a prefix\n",
        "  cloudflare_url = !grep -oE \"https://[a-zA-Z0-9.-]+\\.trycloudflare\\.com\" nohup.out\n",
        "  print(f\"Cloudfare Tunnel Public URL: {cloudflare_url[0]}\")\n",
        "elif Tunnel == \"LocalTunnel\":\n",
        "  !sed -i 's/share=True/share=False/g' $file_path_ui\n",
        "  # install\n",
        "  !npm install -g localtunnel\n",
        "  import time\n",
        "  import urllib\n",
        "  # run localtunnel\n",
        "  with open('url.txt', 'w') as file:\n",
        "        file.write('')\n",
        "\n",
        "  get_ipython().system_raw('lt --port 7860 >> url.txt 2>&1 &')\n",
        "\n",
        "  time.sleep(4)\n",
        "\n",
        "  endpoint_ip = urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\")\n",
        "\n",
        "  with open('url.txt', 'r') as file:\n",
        "      tunnel_url = file.read()\n",
        "      tunnel_url = tunnel_url.replace(\"your url is: \", \"\")\n",
        "\n",
        "  clear_output()\n",
        "  print(f\"LocalTunnel Tunnel Public URL: \\033[0m\\033[93m{tunnel_url}\\033[0m\", end=\"\\033[0m\")\n",
        "\n",
        "  print(f'LocalTunnel Password: {endpoint_ip}')\n",
        "\n",
        "\n",
        "!python app.py"
      ],
      "metadata": {
        "id": "4901EHafGkhK",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}