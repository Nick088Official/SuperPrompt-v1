{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Run SuperPrompt-v1 AI Model NO UI\n",
        "\n",
        "Make your prompts better for AI Art or in general!\n",
        "\n",
        "Used Model: https://huggingface.co/roborovski/superprompt-v1\n",
        "\n",
        "Google Colab Notebook Made by [Nick088](https://linktr.ee/Nick088)"
      ],
      "metadata": {
        "id": "rhAucpX5Li7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install & Load Dependencies, Model\n",
        "\n",
        "#@markdown If you wanna use CPU (slower, no daily limit): Set the CPU from Edit -> Notebook Settings -> CPU\n",
        "\n",
        "#@markdown If you wanna use GPU (faster, max 12 free hours daily limit): Set the Video Card from Edit -> Notebook Settings -> T4 GPU OR Any other GPUs based on your Google Colab Subscription\n",
        "\n",
        "#@markdown Anyways its a very small model, it doesn't matter much if you use cpu or gpu.\n",
        "\n",
        "!pip install transformers\n",
        "!pip install einops\n",
        "!pip install accelerate\n",
        "!pip install sentencepiece\n",
        "import torch\n",
        "from IPython.display import clear_output\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import random\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    print(\"Using GPU\")\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "if Precision_Model_Type == 'fp32':\n",
        "  Precision_Model_Type = 'torch.float32'\n",
        "else:\n",
        "  Precision_Model_Type = 'torch.float16'\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"roborovski/superprompt-v1\", torch_dtype=torch.float16) # torch.float16 is basically fp16, so model precision in 16 bits which is faster and less resource consuming, you could also put torch.float32 which is fp32 that lods it in 32bits which is more precise but slower and more resource consuming\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "clear_output()\n",
        "print(f\"Downloaded & Loaded SuperPrompt-v1 on {'GPU' if device == 'cuda' else 'CPU'}\")"
      ],
      "metadata": {
        "id": "GyK68jfLe5gy",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run Inference\n",
        "#@markdown Write here the prompt you want to upsample to have better detailed description.\n",
        "\n",
        "input_text = \"Expand the following prompt to add more detail: A storefront with 'Text to Image' written on it.\" #@param {type:\"string\"}\n",
        "#@markdown Maximum number of the tokens to generate, controls how long is the text.\n",
        "max_new_tokens = 512 #@param {type:\"slider\", min:250, max:512, step:1}\n",
        "#@markdown Penalize repeated tokens, so make the AI repeat less of itself\n",
        "repetition_penalty = 1.2 #@param {type:\"slider\", min:0.0, max:2.0, step:0.05}\n",
        "#@markdown Higher values produce more diverse outputs\n",
        "temperature = 0.5 #@param {type:\"slider\", min:0.0, max:1.00, step:0.05}\n",
        "#@markdown Higher values sample more low-probability tokens\n",
        "top_p = 1 #@param {type:\"slider\", min:0.0, max:2.0, step:0.05}\n",
        "#@markdown Higher k means more diverse outputs by considering a range of tokens\n",
        "top_k = 1 #@param {type:\"slider\", min:1, max:100, step:1}\n",
        "#@markdown A starting point to initiate the generation process, put 0 for random seed\n",
        "seed = 42 #@param {type:\"integer\"}\n",
        "\n",
        "if seed == 0:\n",
        "    seed = random.randint(1, 100000)\n",
        "    torch.manual_seed(seed)\n",
        "else:\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
        "outputs = model.generate(input_ids, max_new_tokens=max_new_tokens, repetition_penalty=repetition_penalty, do_sample=True, temperature=temperature, top_p=top_p, top_k=top_k)\n",
        "\n",
        "dirty_text = tokenizer.decode(outputs[0])\n",
        "text = dirty_text.replace(\"<pad>\", \"\").replace(\"</s>\", \"\")\n",
        "print(text)"
      ],
      "metadata": {
        "id": "4901EHafGkhK",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}